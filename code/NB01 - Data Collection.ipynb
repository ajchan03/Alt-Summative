{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🎯 Objective\n",
    "\n",
    "This notebook collects Reddit posts and comments from selected subreddits using the Reddit API.  \n",
    "We authenticate, extract posts and comments, and save them in JSON format for further analysis.  \n",
    "\n",
    "---\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 📚 Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # File and path handling\n",
    "import time  # Delays to prevent API rate limits\n",
    "import json  # Saving and reading JSON files\n",
    "import random  # Random selection of comments\n",
    "import praw  # Reddit API wrapper\n",
    "from dotenv import load_dotenv  # Securely load API credentials\n",
    "from datetime import datetime  # Handle timestamps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 🔍 Research Design\n",
    "\n",
    "### **Research Question:**\n",
    "How do discussions around political topics differ between left-leaning and right-leaning subreddits?\n",
    "\n",
    "### **Why These Subreddits?**\n",
    "We selected:\n",
    "- **r/politics** → A left-leaning political subreddit.\n",
    "- **r/Conservative** → A right-leaning political subreddit.\n",
    "\n",
    "This helps ensure **balanced political discourse** from both perspectives.\n",
    "\n",
    "### **Data Points Collected**\n",
    "For each **post**, we collect:\n",
    "- `id` → Unique Reddit post ID\n",
    "- `subreddit` → Subreddit name\n",
    "- `title` → Title of the post\n",
    "- `score` → Upvote count\n",
    "- `num_comments` → Number of comments\n",
    "- `created_utc` → Timestamp of creation\n",
    "- `text` → Post content\n",
    "- `url` → Link to the post\n",
    "\n",
    "For each **comment**, we collect:\n",
    "- `comment_id` → Unique ID of the comment\n",
    "- `post_id` → ID of the related post\n",
    "- `body` → Comment text\n",
    "- `score` → Upvote count\n",
    "- `created_utc` → Timestamp of creation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 💻 API Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 💻 Reddit API Authentication\n",
    "def authenticate_reddit():\n",
    "    \"\"\"\n",
    "    Authenticate with the Reddit API using credentials stored in environment variables.\n",
    "\n",
    "    Returns:\n",
    "        praw.Reddit: An authenticated Reddit API client instance.\n",
    "    \"\"\"\n",
    "    load_dotenv()\n",
    "    return praw.Reddit(\n",
    "        client_id=os.getenv(\"REDDIT_CLIENT_ID\"),\n",
    "        client_secret=os.getenv(\"REDDIT_CLIENT_SECRET\"),\n",
    "        user_agent=\"ElectionScraper\"\n",
    "    )\n",
    "\n",
    "\n",
    "\n",
    "# Fetch Posts (Ensure No Duplicates)\n",
    "def fetch_posts(reddit, subreddits, limit=50):\n",
    "    \"\"\"\n",
    "    Fetches top posts from selected subreddits while ensuring no duplicates.\n",
    "\n",
    "    Args:\n",
    "        reddit (praw.Reddit): Authenticated Reddit API client.\n",
    "        subreddits (list): List of subreddit names.\n",
    "        limit (int, optional): Number of posts per subreddit.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of unique dictionaries containing post details.\n",
    "    \"\"\"\n",
    "    start_date = datetime(2024, 3, 1)\n",
    "    end_date = datetime(2025, 1, 6)\n",
    "    \n",
    "    collected_posts = {}\n",
    "    \n",
    "    for subreddit in subreddits:\n",
    "        print(f\"🔄 Fetching top posts from r/{subreddit}...\")\n",
    "\n",
    "        subreddit_obj = reddit.subreddit(subreddit)\n",
    "        total_fetched = 0\n",
    "        temp_posts = []\n",
    "        \n",
    "        while len(temp_posts) < limit:\n",
    "            print(f\"📊 Fetching posts from r/{subreddit}... (Total so far: {total_fetched})\")\n",
    "\n",
    "            for post in subreddit_obj.top(time_filter=\"year\", limit=500):\n",
    "                total_fetched += 1\n",
    "\n",
    "                if (post.id not in collected_posts and  # Ensure post is not already saved\n",
    "                    post.num_comments >= 300 and\n",
    "                    start_date <= datetime.utcfromtimestamp(post.created_utc) <= end_date and\n",
    "                    not post.is_video and \n",
    "                    not post.url.endswith(('.gif', '.jpg', '.png', '.mp4', '.webm')) and \n",
    "                    len(post.selftext.split()) >= 5):  # Ensure meaningful text\n",
    "\n",
    "                    # Store unique post\n",
    "                    collected_posts[post.id] = {\n",
    "                        \"id\": post.id,\n",
    "                        \"subreddit\": subreddit,\n",
    "                        \"title\": post.title,\n",
    "                        \"score\": post.score,\n",
    "                        \"num_comments\": post.num_comments,\n",
    "                        \"created_utc\": datetime.utcfromtimestamp(post.created_utc).isoformat(),\n",
    "                        \"text\": post.selftext.strip(),\n",
    "                        \"url\": post.url\n",
    "                    }\n",
    "\n",
    "                    temp_posts.append(post.id)  # Track posts per subreddit\n",
    "                    \n",
    "                    if len(temp_posts) >= limit:\n",
    "                        break  # Stop when reaching the limit\n",
    "\n",
    "                time.sleep(0.05)  # ⏳ Prevent rate-limiting\n",
    "            \n",
    "            if total_fetched >= 5000:  # Prevent infinite loops\n",
    "                print(f\"⚠️ Stopping search for r/{subreddit}, reached 5000 posts with only {len(temp_posts)} valid.\")\n",
    "                break\n",
    "\n",
    "        print(f\"✅ Collected {len(temp_posts)} unique posts from r/{subreddit}\")\n",
    "\n",
    "    return list(collected_posts.values())  # Convert dictionary back to list\n",
    "\n",
    "\n",
    "# 💻 Fetch 300 Random Comments per Post\n",
    "def fetch_random_comments(reddit, post_id, limit=300):\n",
    "    \"\"\"\n",
    "    Fetches 300 random top-level comments for a given Reddit post.\n",
    "\n",
    "    Args:\n",
    "        reddit (praw.Reddit): Authenticated Reddit API client.\n",
    "        post_id (str): ID of the post to fetch comments for.\n",
    "        limit (int, optional): Number of comments to retrieve.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries containing comment details.\n",
    "    \"\"\"\n",
    "    post = reddit.submission(id=post_id)\n",
    "\n",
    "    try:\n",
    "        post.comments.replace_more(limit=0)  # Load all top-level comments\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error fetching comments for {post_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "    all_comments = [\n",
    "        comment for comment in post.comments\n",
    "        if comment.author and comment.author.name.lower() != \"automoderator\" and len(comment.body.split()) >= 3\n",
    "    ]\n",
    "\n",
    "    random_comments = random.sample(all_comments, min(len(all_comments), limit))\n",
    "\n",
    "    return [{\n",
    "        \"post_id\": post_id,\n",
    "        \"comment_id\": comment.id,\n",
    "        \"body\": comment.body,\n",
    "        \"score\": comment.score,\n",
    "        \"created_utc\": datetime.utcfromtimestamp(comment.created_utc).isoformat()\n",
    "    } for comment in random_comments]\n",
    "    \n",
    "\n",
    "\n",
    "# 💾 Save Data as JSON\n",
    "def save_to_json(data, filename):\n",
    "    \"\"\"\n",
    "    Saves a list of dictionaries as a JSON file in the specified directory.\n",
    "\n",
    "    Args:\n",
    "        data (list): The data to save.\n",
    "        filename (str): The filename to save the data to.\n",
    "\n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    folder = \"/files/ds105a-2024-alternative-summative-ajchan03/data/raw/\"\n",
    "    os.makedirs(folder, exist_ok=True)  # Ensure the directory exists\n",
    "    file_path = os.path.join(folder, filename)\n",
    "\n",
    "    with open(file_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(data, f, indent=4)\n",
    "\n",
    "    print(f\"📂 Saved {filename} in {folder}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 📥 Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔄 Fetching top posts from r/politics...\n",
      "📊 Fetching posts from r/politics... (Total so far: 0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Fetching posts from r/politics... (Total so far: 500)\n",
      "📊 Fetching posts from r/politics... (Total so far: 1000)\n",
      "📊 Fetching posts from r/politics... (Total so far: 1500)\n",
      "📊 Fetching posts from r/politics... (Total so far: 2000)\n",
      "📊 Fetching posts from r/politics... (Total so far: 2500)\n",
      "📊 Fetching posts from r/politics... (Total so far: 3000)\n",
      "📊 Fetching posts from r/politics... (Total so far: 3500)\n",
      "📊 Fetching posts from r/politics... (Total so far: 4000)\n",
      "📊 Fetching posts from r/politics... (Total so far: 4500)\n",
      "⚠️ Stopping search for r/politics, reached 5000 posts with only 7 valid.\n",
      "✅ Collected 7 unique posts from r/politics\n",
      "🔄 Fetching top posts from r/Conservative...\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 0)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 500)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 1000)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 1500)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 2000)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 2500)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 3000)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 3500)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 4000)\n",
      "📊 Fetching posts from r/Conservative... (Total so far: 4500)\n",
      "⚠️ Stopping search for r/Conservative, reached 5000 posts with only 27 valid.\n",
      "✅ Collected 27 unique posts from r/Conservative\n",
      "🔄 Fetching top posts from r/PoliticalDiscussion...\n",
      "📊 Fetching posts from r/PoliticalDiscussion... (Total so far: 0)\n",
      "✅ Collected 50 unique posts from r/PoliticalDiscussion\n",
      "🔄 Fetching top posts from r/SandersForPresident...\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 0)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 500)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 1000)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 1500)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 2000)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 2500)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 3000)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 3500)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 4000)\n",
      "📊 Fetching posts from r/SandersForPresident... (Total so far: 4500)\n",
      "⚠️ Stopping search for r/SandersForPresident, reached 5000 posts with only 5 valid.\n",
      "✅ Collected 5 unique posts from r/SandersForPresident\n",
      "✅ Collected 89 posts after filtering.\n",
      "📥 Fetching 300 comments for post 1d4emcb...\n",
      "📥 Fetching 300 comments for post 1elhbeb...\n",
      "📥 Fetching 300 comments for post 1e8sabh...\n",
      "📥 Fetching 300 comments for post 1dsupsh...\n",
      "📥 Fetching 300 comments for post 1e2n1kr...\n",
      "📥 Fetching 300 comments for post 1e3vswh...\n",
      "📥 Fetching 300 comments for post 1ez1xkp...\n",
      "📥 Fetching 300 comments for post 1gl62gg...\n",
      "📥 Fetching 300 comments for post 1gl393m...\n",
      "📥 Fetching 300 comments for post 1gktlf7...\n",
      "📥 Fetching 300 comments for post 1gl0v8t...\n",
      "📥 Fetching 300 comments for post 1gkxt46...\n",
      "📥 Fetching 300 comments for post 1glpluw...\n",
      "📥 Fetching 300 comments for post 1gkdqa1...\n",
      "📥 Fetching 300 comments for post 1gkrfb1...\n",
      "📥 Fetching 300 comments for post 1gl2cnc...\n",
      "📥 Fetching 300 comments for post 1gkzqlc...\n",
      "📥 Fetching 300 comments for post 1f11hcx...\n",
      "📥 Fetching 300 comments for post 1gmb1x3...\n",
      "📥 Fetching 300 comments for post 1gmaq9d...\n",
      "📥 Fetching 300 comments for post 1hgkau0...\n",
      "📥 Fetching 300 comments for post 1glelw5...\n",
      "📥 Fetching 300 comments for post 1gfvtto...\n",
      "📥 Fetching 300 comments for post 1e9965b...\n",
      "📥 Fetching 300 comments for post 1gl66bd...\n",
      "📥 Fetching 300 comments for post 1glwarc...\n",
      "📥 Fetching 300 comments for post 1gnmt76...\n",
      "📥 Fetching 300 comments for post 1gjlya8...\n",
      "📥 Fetching 300 comments for post 1glttrw...\n",
      "📥 Fetching 300 comments for post 1gr6o39...\n",
      "📥 Fetching 300 comments for post 1exuzkh...\n",
      "📥 Fetching 300 comments for post 1fidisr...\n",
      "📥 Fetching 300 comments for post 1go5yna...\n",
      "📥 Fetching 300 comments for post 1gp5lys...\n",
      "📥 Fetching 300 comments for post 1gksts4...\n",
      "📥 Fetching 300 comments for post 1ef5frf...\n",
      "📥 Fetching 300 comments for post 1gi79lt...\n",
      "📥 Fetching 300 comments for post 1bkq6ra...\n",
      "📥 Fetching 300 comments for post 1emqyex...\n",
      "📥 Fetching 300 comments for post 1et5yej...\n",
      "📥 Fetching 300 comments for post 1ep2p7z...\n",
      "📥 Fetching 300 comments for post 1e8s8mb...\n",
      "📥 Fetching 300 comments for post 1e9qr62...\n",
      "📥 Fetching 300 comments for post 1fijare...\n",
      "📥 Fetching 300 comments for post 1d21c1o...\n",
      "📥 Fetching 300 comments for post 1dfxc4a...\n",
      "📥 Fetching 300 comments for post 1eurk6t...\n",
      "📥 Fetching 300 comments for post 1enhoeu...\n",
      "📥 Fetching 300 comments for post 1bqerw6...\n",
      "📥 Fetching 300 comments for post 1gfepbg...\n",
      "📥 Fetching 300 comments for post 1hpb5d0...\n",
      "📥 Fetching 300 comments for post 1ekcv65...\n",
      "📥 Fetching 300 comments for post 1elh7a4...\n",
      "📥 Fetching 300 comments for post 1gb3oj9...\n",
      "📥 Fetching 300 comments for post 1enhhpw...\n",
      "📥 Fetching 300 comments for post 1chu3ir...\n",
      "📥 Fetching 300 comments for post 1e7f3m1...\n",
      "📥 Fetching 300 comments for post 1dozw2c...\n",
      "📥 Fetching 300 comments for post 1frmi5h...\n",
      "📥 Fetching 300 comments for post 1ebch4v...\n",
      "📥 Fetching 300 comments for post 1gqo32j...\n",
      "📥 Fetching 300 comments for post 1fe1787...\n",
      "📥 Fetching 300 comments for post 1grgblk...\n",
      "📥 Fetching 300 comments for post 1ejjmm3...\n",
      "📥 Fetching 300 comments for post 1eiit1q...\n",
      "📥 Fetching 300 comments for post 1euwq6y...\n",
      "📥 Fetching 300 comments for post 1eahe77...\n",
      "📥 Fetching 300 comments for post 1dzj2pq...\n",
      "📥 Fetching 300 comments for post 1eu0aju...\n",
      "📥 Fetching 300 comments for post 1e19099...\n",
      "📥 Fetching 300 comments for post 1g1qs25...\n",
      "📥 Fetching 300 comments for post 1gjy0s5...\n",
      "📥 Fetching 300 comments for post 1fdzt7t...\n",
      "📥 Fetching 300 comments for post 1f2un7j...\n",
      "📥 Fetching 300 comments for post 1e2n207...\n",
      "📥 Fetching 300 comments for post 1cgxhhe...\n",
      "📥 Fetching 300 comments for post 1e3w1ey...\n",
      "📥 Fetching 300 comments for post 1gehaga...\n",
      "📥 Fetching 300 comments for post 1dq9k8e...\n",
      "📥 Fetching 300 comments for post 1d93xxi...\n",
      "📥 Fetching 300 comments for post 1g3p0z6...\n",
      "📥 Fetching 300 comments for post 1dxnja2...\n",
      "📥 Fetching 300 comments for post 1eyp5ml...\n",
      "📥 Fetching 300 comments for post 1e5ldet...\n",
      "📥 Fetching 300 comments for post 1bdzhmj...\n",
      "📥 Fetching 300 comments for post 1e7wuf2...\n",
      "📥 Fetching 300 comments for post 1eb7xj1...\n",
      "📥 Fetching 300 comments for post 1fdmtwa...\n",
      "📥 Fetching 300 comments for post 1ep8803...\n",
      "📂 Saved reddit_filtered_posts.json in /files/ds105a-2024-alternative-summative-ajchan03/data/raw/\n",
      "📂 Saved reddit_filtered_comments.json in /files/ds105a-2024-alternative-summative-ajchan03/data/raw/\n",
      "✅ Data collection complete! JSON files saved.\n"
     ]
    }
   ],
   "source": [
    "# 📥 Step 1: Authenticate Reddit API\n",
    "reddit = authenticate_reddit()\n",
    "subreddits = [\"politics\", \"Conservative\", \"PoliticalDiscussion\", \"SandersForPresident\"]\n",
    "\n",
    "# 📥 Step 2: Fetch Posts\n",
    "all_posts = fetch_posts(reddit, subreddits, limit=50)\n",
    "print(f\"✅ Collected {len(all_posts)} posts after filtering.\")\n",
    "\n",
    "# 📥 Step 3: Fetch Comments\n",
    "all_comments = []\n",
    "for post in all_posts:\n",
    "    print(f\"📥 Fetching 300 comments for post {post['id']}...\")\n",
    "    comments = fetch_random_comments(reddit, post[\"id\"], limit=300)\n",
    "    all_comments.extend(comments)\n",
    "\n",
    "# 📥 Step 4: Save Data\n",
    "save_to_json(all_posts, \"reddit_filtered_posts.json\")\n",
    "save_to_json(all_comments, \"reddit_filtered_comments.json\")\n",
    "\n",
    "print(\"✅ Data collection complete! JSON files saved.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ✅ Quality Check\n",
    "\n",
    "Before using the collected data, we verify:\n",
    "- JSON files exist in `/data/raw/`\n",
    "- The collected data has expected structure\n",
    "- The number of posts and comments is reasonable\n",
    "\n",
    "By performing this step, we ensure **high-quality data** before further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔️ JSON files successfully saved in /files/ds105a-2024-alternative-summative-ajchan03/data/raw/\n"
     ]
    }
   ],
   "source": [
    "# ✅ Check if JSON Files Exist in the Correct Directory\n",
    "post_file = \"/files/ds105a-2024-alternative-summative-ajchan03/data/raw/reddit_filtered_posts.json\"\n",
    "comment_file = \"/files/ds105a-2024-alternative-summative-ajchan03/data/raw/reddit_filtered_comments.json\"\n",
    "\n",
    "if os.path.exists(post_file) and os.path.exists(comment_file):\n",
    "    print(\"✔️ JSON files successfully saved in /files/ds105a-2024-alternative-summative-ajchan03/data/raw/\")\n",
    "else:\n",
    "    print(\"❌ Error: Files not found!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
