{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Objective\n",
    "\n",
    "This notebook processes raw Reddit data collected in NB01, cleans it, and stores it in a structured SQLite database.  \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import json  \n",
    "import sqlite3  \n",
    "import pandas as pd  \n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Define correct file paths\n",
    "BASE_DIR = \"/files/ds105a-2024-alternative-summative-ajchan03\"  \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")  \n",
    "\n",
    "# Define file paths\n",
    "POSTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_posts.json\")\n",
    "COMMENTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_comments.json\")\n",
    "\n",
    "# Check if JSON files exist before attempting to load them\n",
    "if not os.path.exists(POSTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{POSTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "if not os.path.exists(COMMENTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{COMMENTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "# Load JSON data into DataFrames\n",
    "with open(POSTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_data = json.load(f)\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "\n",
    "with open(COMMENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    comments_data = json.load(f)\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Data successfully loaded\n",
    "print(\"Data Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning & Transformation\n",
    "\n",
    "This section does the following:\n",
    "- Handling **missing values**  \n",
    "- Converting timestamps to **datetime format**  \n",
    "- Removing **duplicate posts and comments**  \n",
    "- Ensuring all comments **link to a valid post**  \n",
    "- **Filters comments so that only ones containing 'Trump' remain** to match our research question\n",
    "- Adding **sentiment analysis** to comments  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/datahub/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id comment_id                                               body  \\\n",
      "0  1d4emcb    l6e9yus  So all those who still believe James Buchanan ...   \n",
      "2  1d4emcb    l6e9bug  My favorite is Fox News saying this was all or...   \n",
      "3  1d4emcb    l6dqpw7         Just here for HISTORY!!!!\\n\\n\\nFUCK TRUMP.   \n",
      "5  1d4emcb    l6dt95e  Trump is number 1!!!  First to lose the popula...   \n",
      "6  1d4emcb    l6dti72  His Wikipedia page has been updated as well.¬†¬†...   \n",
      "\n",
      "   score         created_utc subreddit  comment_sentiment  \n",
      "0    113 2024-05-30 23:08:29  politics            -0.7717  \n",
      "2    145 2024-05-30 23:04:25  politics            -0.5859  \n",
      "3    994 2024-05-30 21:12:19  politics            -0.7507  \n",
      "5     57 2024-05-30 21:26:54  politics            -0.7887  \n",
      "6    110 2024-05-30 21:28:21  politics             0.7783  \n",
      "\n",
      "‚úîÔ∏è No duplicate posts found.\n",
      "\n",
      "‚úîÔ∏è No duplicate comments found.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Step 2: Data Cleaning (Filtering for Trump-Related Comments)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK VADER sentiment analysis tool\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# üßπ Handle Missing Values\n",
    "df_posts.fillna(\"\", inplace=True)\n",
    "df_comments.fillna(\"\", inplace=True)\n",
    "\n",
    "# Convert Date Fields to Datetime\n",
    "df_posts[\"created_utc\"] = pd.to_datetime(df_posts[\"created_utc\"])\n",
    "df_comments[\"created_utc\"] = pd.to_datetime(df_comments[\"created_utc\"])\n",
    "\n",
    "# Add 'subreddit' to Comments Table\n",
    "df_comments = df_comments.merge(df_posts[['id', 'subreddit']], left_on=\"post_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "# Drop the duplicate 'id' column (from df_posts) since 'post_id' is already in df_comments\n",
    "df_comments.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# Remove Duplicate Posts\n",
    "df_posts.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "\n",
    "# Remove Duplicate Comments\n",
    "df_comments.drop_duplicates(subset=[\"comment_id\"], inplace=True)\n",
    "\n",
    "# Ensure Foreign Key Consistency\n",
    "df_comments = df_comments[df_comments[\"post_id\"].isin(df_posts[\"id\"])]\n",
    "\n",
    "# Filter Comments: Keep only those mentioning \"Trump\" (case-insensitive)\n",
    "df_comments = df_comments[df_comments[\"body\"].str.contains(r'\\bTrump\\b', flags=re.IGNORECASE, regex=True, na=False)]\n",
    "\n",
    "# Add Sentiment Analysis\n",
    "df_comments[\"comment_sentiment\"] = df_comments[\"body\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# Ensure No Duplicates in DataFrames\n",
    "duplicate_posts = df_posts[df_posts.duplicated(subset=[\"id\"], keep=False)]\n",
    "duplicate_comments = df_comments[df_comments.duplicated(subset=[\"comment_id\"], keep=False)]\n",
    "\n",
    "\n",
    "print(df_comments.head())\n",
    "\n",
    "if duplicate_posts.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate posts found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate posts detected!\")\n",
    "    print(duplicate_posts)\n",
    "\n",
    "if duplicate_comments.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate comments found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate comments detected!\")\n",
    "    print(duplicate_comments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ Database Design\n",
    "\n",
    "This seciton does the following:\n",
    "- **Define the SQLite database structure**\n",
    "- **Create tables (`posts` & `comments`) with foreign key relationships**\n",
    "- **Store the cleaned data into the database**\n",
    "\n",
    "### \n",
    "**Database Structure**\n",
    "\n",
    "I store the data in **`data/reddit_data.db`**.\n",
    "\n",
    "| **Table**   | **Columns** | **Primary Key** | **Foreign Key** |\n",
    "|------------|------------|----------------|----------------|\n",
    "| **posts**  | `id, subreddit, title, score, num_comments, created_utc, text, url` | `id` | - |\n",
    "| **comments** | `comment_id, post_id, body, score, created_utc, comment_sentiment` | `comment_id` | `post_id` (FK ‚Üí posts.id) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Database Creation & Data Insertion Completed!\n"
     ]
    }
   ],
   "source": [
    "# üì• Step 4: Database Creation\n",
    "\n",
    "# Define database path\n",
    "DB_PATH = os.path.join(BASE_DIR, \"data\", \"reddit_data.db\")\n",
    "\n",
    "# Connect to SQLite and create tables\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create 'posts' table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS posts (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    subreddit TEXT,\n",
    "    title TEXT,\n",
    "    score INTEGER,\n",
    "    num_comments INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    text TEXT,\n",
    "    url TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create 'comments' table with sentiment analysis\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS comments (\n",
    "    comment_id TEXT PRIMARY KEY,\n",
    "    post_id TEXT,\n",
    "    body TEXT,\n",
    "    score INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    comment_sentiment REAL,\n",
    "    FOREIGN KEY (post_id) REFERENCES posts (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into SQLite database\n",
    "df_posts.to_sql(\"posts\", conn, if_exists=\"replace\", index=False)\n",
    "df_comments.to_sql(\"comments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ‚úÖ Quality Check\n",
    "\n",
    "The code does the following:\n",
    "- Checking record counts\n",
    "- Validating foreign key relationships\n",
    "- Inspecting sentiment score distribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Table Row Counts:\n",
      "Posts: 841\n",
      "Comments: 19132\n",
      "\n",
      "‚úÖ Foreign Key Check: All comments have valid posts.\n",
      "\n",
      "‚úÖ Data Quality Check Completed!\n"
     ]
    }
   ],
   "source": [
    "# Check Table Row Counts\n",
    "print(\"\\nüìä Table Row Counts:\")\n",
    "print(\"Posts:\", pd.read_sql_query(\"SELECT COUNT(*) FROM posts;\", conn).iloc[0, 0])\n",
    "print(\"Comments:\", pd.read_sql_query(\"SELECT COUNT(*) FROM comments;\", conn).iloc[0, 0])\n",
    "\n",
    "# Validate Foreign Keys (Ensure All Comments Link to Valid Posts)\n",
    "invalid_comments = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) FROM comments c\n",
    "    LEFT JOIN posts p ON c.post_id = p.id\n",
    "    WHERE p.id IS NULL;\n",
    "\"\"\", conn).iloc[0, 0]\n",
    "\n",
    "if invalid_comments == 0:\n",
    "    print(\"\\n‚úÖ Foreign Key Check: All comments have valid posts.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {invalid_comments} comments have no associated post!\")\n",
    "\n",
    "\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
