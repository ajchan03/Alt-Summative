{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Objective\n",
    "This notebook processes raw Reddit JSON data, cleans it, and saves it into a structured SQLite database. \n",
    "We will create two linked tables: 'posts' and 'comments'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # File operations\n",
    "import json  # Handling JSON data\n",
    "import sqlite3  # Database storage\n",
    "import pandas as pd  # Data processing & transformation\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Define correct file paths\n",
    "BASE_DIR = \"/files/ds105a-2024-alternative-summative-ajchan03\"  \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")  \n",
    "\n",
    "# Define file paths\n",
    "POSTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_posts.json\")\n",
    "COMMENTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_comments.json\")\n",
    "\n",
    "# Check if JSON files exist before attempting to load them\n",
    "if not os.path.exists(POSTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{POSTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "if not os.path.exists(COMMENTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{COMMENTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "# Load JSON data into DataFrames\n",
    "with open(POSTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_data = json.load(f)\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "\n",
    "with open(COMMENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    comments_data = json.load(f)\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Data successfully loaded\n",
    "print(\"Data Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning & Transformation\n",
    "\n",
    "In this step, we **clean and transform** our data by:\n",
    "- Handling **missing values**  \n",
    "- Converting timestamps to **datetime format**  \n",
    "- Removing **duplicates**  \n",
    "- Ensuring all comments **link to a valid post**  \n",
    "- Adding **sentiment scores** using **NLTK's VADER**  \n",
    "- **Filtering comments** to include **only those mentioning \"Trump\"**  \n",
    "\n",
    "By the end of this step, our dataset will be **structured, focused, and ready for analysis**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/datahub/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id comment_id                                               body  \\\n",
      "0  1d4emcb    l6dr6td  Congratulations, Donald Trump. \\n\\nYou have th...   \n",
      "1  1d4emcb    l6dw3k9  r/Conservative can finally celebrate Trump bei...   \n",
      "2  1d4emcb    l6dzp5m  From the NBC live feed, they interviewed an 83...   \n",
      "4  1d4emcb    l6e90j9  Thank you to the brave 12 people who remained ...   \n",
      "8  1d4emcb    l6elbyz  Some conservatives and republicans are saying ...   \n",
      "\n",
      "   score         created_utc  comment_sentiment  \n",
      "0   5087 2024-05-30 21:14:59             0.8353  \n",
      "1     40 2024-05-30 21:43:19             0.9117  \n",
      "2     44 2024-05-30 22:04:41             0.4310  \n",
      "4     44 2024-05-30 23:02:26             0.8807  \n",
      "8     51 2024-05-31 00:22:24            -0.8402  \n",
      "\n",
      "‚úîÔ∏è No duplicate posts found.\n",
      "\n",
      "‚úîÔ∏è No duplicate comments found.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Step 2: Data Cleaning (Filtering for Trump-Related Comments)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK VADER sentiment analysis tool\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# üßπ Handle Missing Values\n",
    "df_posts.fillna(\"\", inplace=True)\n",
    "df_comments.fillna(\"\", inplace=True)\n",
    "\n",
    "# ‚úÖ Convert Date Fields to Datetime\n",
    "df_posts[\"created_utc\"] = pd.to_datetime(df_posts[\"created_utc\"])\n",
    "df_comments[\"created_utc\"] = pd.to_datetime(df_comments[\"created_utc\"])\n",
    "\n",
    "# ‚úÖ Remove Duplicate Posts\n",
    "df_posts.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Remove Duplicate Comments\n",
    "df_comments.drop_duplicates(subset=[\"comment_id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Ensure Foreign Key Consistency\n",
    "df_comments = df_comments[df_comments[\"post_id\"].isin(df_posts[\"id\"])]\n",
    "\n",
    "# ‚úÖ Filter Comments: Keep only those mentioning \"Trump\" (case-insensitive)\n",
    "df_comments = df_comments[df_comments[\"body\"].str.contains(r'\\bTrump\\b', flags=re.IGNORECASE, regex=True, na=False)]\n",
    "\n",
    "# ‚úÖ Add Sentiment Analysis\n",
    "df_comments[\"comment_sentiment\"] = df_comments[\"body\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# Ensure No Duplicates in DataFrames\n",
    "duplicate_posts = df_posts[df_posts.duplicated(subset=[\"id\"], keep=False)]\n",
    "duplicate_comments = df_comments[df_comments.duplicated(subset=[\"comment_id\"], keep=False)]\n",
    "\n",
    "\n",
    "print(df_comments.head())\n",
    "\n",
    "if duplicate_posts.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate posts found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate posts detected!\")\n",
    "    print(duplicate_posts)\n",
    "\n",
    "if duplicate_comments.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate comments found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate comments detected!\")\n",
    "    print(duplicate_comments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ Database Design\n",
    "\n",
    "Now that our **data is cleaned** and **sentiment scores are added**, we will:\n",
    "- **Define the SQLite database structure**\n",
    "- **Create tables (`posts` & `comments`) with foreign key relationships**\n",
    "- **Store the cleaned data into the database**\n",
    "\n",
    "### \n",
    "**Database Structure**\n",
    "We will store the data in **`data/reddit_data.db`**.\n",
    "\n",
    "| **Table**   | **Columns** | **Primary Key** | **Foreign Key** |\n",
    "|------------|------------|----------------|----------------|\n",
    "| **posts**  | `id, subreddit, title, score, num_comments, created_utc, text, url` | `id` | - |\n",
    "| **comments** | `comment_id, post_id, body, score, created_utc, comment_sentiment` | `comment_id` | `post_id` (FK ‚Üí posts.id) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Database Creation & Data Insertion Completed!\n"
     ]
    }
   ],
   "source": [
    "# üì• Step 4: Database Creation\n",
    "\n",
    "# Define database path\n",
    "DB_PATH = os.path.join(BASE_DIR, \"data\", \"reddit_data.db\")\n",
    "\n",
    "# Connect to SQLite and create tables\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create 'posts' table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS posts (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    subreddit TEXT,\n",
    "    title TEXT,\n",
    "    score INTEGER,\n",
    "    num_comments INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    text TEXT,\n",
    "    url TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create 'comments' table with sentiment analysis\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS comments (\n",
    "    comment_id TEXT PRIMARY KEY,\n",
    "    post_id TEXT,\n",
    "    body TEXT,\n",
    "    score INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    comment_sentiment REAL,\n",
    "    FOREIGN KEY (post_id) REFERENCES posts (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into SQLite database\n",
    "df_posts.to_sql(\"posts\", conn, if_exists=\"replace\", index=False)\n",
    "df_comments.to_sql(\"comments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\n‚úÖ Database Creation & Data Insertion Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check\n",
    "\n",
    "Before moving to analysis, we ensure **database integrity** by:\n",
    "- Checking record counts\n",
    "- Validating foreign key relationships\n",
    "- Inspecting sentiment score distribution\n",
    "\n",
    "This ensures **clean, structured data** for analysis in NB03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Table Row Counts:\n",
      "Posts: 34\n",
      "Comments: 686\n",
      "\n",
      "‚úÖ Foreign Key Check: All comments have valid posts.\n",
      "\n",
      "‚úÖ Data Quality Check Completed!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 5: Quality Check\n",
    "\n",
    "# Check Table Row Counts\n",
    "print(\"\\nüìä Table Row Counts:\")\n",
    "print(\"Posts:\", pd.read_sql_query(\"SELECT COUNT(*) FROM posts;\", conn).iloc[0, 0])\n",
    "print(\"Comments:\", pd.read_sql_query(\"SELECT COUNT(*) FROM comments;\", conn).iloc[0, 0])\n",
    "\n",
    "# Validate Foreign Keys (Ensure All Comments Link to Valid Posts)\n",
    "invalid_comments = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) FROM comments c\n",
    "    LEFT JOIN posts p ON c.post_id = p.id\n",
    "    WHERE p.id IS NULL;\n",
    "\"\"\", conn).iloc[0, 0]\n",
    "\n",
    "if invalid_comments == 0:\n",
    "    print(\"\\n‚úÖ Foreign Key Check: All comments have valid posts.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {invalid_comments} comments have no associated post!\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Data Quality Check Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
