{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Objective\n",
    "\n",
    "This notebook processes raw Reddit data collected in NB01, cleans it, and stores it in a structured SQLite database.  \n",
    "We are analyzing discussions from three different subreddits:  \n",
    "- **r/politics** (left-leaning)\n",
    "- **r/Conservative** (right-leaning)\n",
    "- **r/PoliticalDiscussion** (moderated, neutral discourse)\n",
    "\n",
    "By the end of this notebook, we will have **a structured dataset** ready for analysis in **NB03**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # File operations\n",
    "import json  # Handling JSON data\n",
    "import sqlite3  # Database storage\n",
    "import pandas as pd  # Data processing & transformation\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Define correct file paths\n",
    "BASE_DIR = \"/files/ds105a-2024-alternative-summative-ajchan03\"  \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")  \n",
    "\n",
    "# Define file paths\n",
    "POSTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_posts.json\")\n",
    "COMMENTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_comments.json\")\n",
    "\n",
    "# Check if JSON files exist before attempting to load them\n",
    "if not os.path.exists(POSTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{POSTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "if not os.path.exists(COMMENTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{COMMENTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "# Load JSON data into DataFrames\n",
    "with open(POSTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_data = json.load(f)\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "\n",
    "with open(COMMENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    comments_data = json.load(f)\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Data successfully loaded\n",
    "print(\"Data Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning & Transformation\n",
    "\n",
    "We ensure our dataset is **clean and structured** by:\n",
    "- Handling **missing values**  \n",
    "- Converting timestamps to **datetime format**  \n",
    "- Removing **duplicate posts and comments**  \n",
    "- Ensuring all comments **link to a valid post**  \n",
    "- **Filtering only posts from `r/politics`, `r/Conservative`, and `r/PoliticalDiscussion`**  \n",
    "- Adding **sentiment analysis** to comments  \n",
    "\n",
    "By the end of this step, our dataset will be **structured for database storage**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/datahub/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    post_id comment_id                                               body  \\\n",
      "7   1d4emcb    l6drhd7  You talking about twice impeached, convicted f...   \n",
      "8   1d4emcb    l6dycuu  Kinda crazy to think that Donald Trump had com...   \n",
      "11  1d4emcb    l6dwst2  We can now officially add \"convicted felon\" to...   \n",
      "12  1d4emcb    l6dueig  Wooo a 34/34, perfect 100%! Highest grade Trum...   \n",
      "19  1d4emcb    l6dvis0  The first line of his wikipedia page (not by m...   \n",
      "\n",
      "    score         created_utc subreddit  comment_sentiment  \n",
      "7    1246 2024-05-30 21:16:40  politics             0.0000  \n",
      "8      39 2024-05-30 21:56:35  politics            -0.5434  \n",
      "11     36 2024-05-30 21:47:25  politics             0.0000  \n",
      "12     72 2024-05-30 21:33:33  politics             0.6114  \n",
      "19     74 2024-05-30 21:39:59  politics             0.4215  \n",
      "\n",
      "‚úîÔ∏è No duplicate posts found.\n",
      "\n",
      "‚úîÔ∏è No duplicate comments found.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Step 2: Data Cleaning (Filtering for Trump-Related Comments)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK VADER sentiment analysis tool\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# üßπ Handle Missing Values\n",
    "df_posts.fillna(\"\", inplace=True)\n",
    "df_comments.fillna(\"\", inplace=True)\n",
    "\n",
    "# ‚úÖ Convert Date Fields to Datetime\n",
    "df_posts[\"created_utc\"] = pd.to_datetime(df_posts[\"created_utc\"])\n",
    "df_comments[\"created_utc\"] = pd.to_datetime(df_comments[\"created_utc\"])\n",
    "\n",
    "# Add 'subreddit' to Comments Table\n",
    "df_comments = df_comments.merge(df_posts[['id', 'subreddit']], left_on=\"post_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "# Drop the duplicate 'id' column (from df_posts) since 'post_id' is already in df_comments\n",
    "df_comments.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Remove Duplicate Posts\n",
    "df_posts.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Remove Duplicate Comments\n",
    "df_comments.drop_duplicates(subset=[\"comment_id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Ensure Foreign Key Consistency\n",
    "df_comments = df_comments[df_comments[\"post_id\"].isin(df_posts[\"id\"])]\n",
    "\n",
    "# ‚úÖ Filter Comments: Keep only those mentioning \"Trump\" (case-insensitive)\n",
    "df_comments = df_comments[df_comments[\"body\"].str.contains(r'\\bTrump\\b', flags=re.IGNORECASE, regex=True, na=False)]\n",
    "\n",
    "# ‚úÖ Add Sentiment Analysis\n",
    "df_comments[\"comment_sentiment\"] = df_comments[\"body\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# Ensure No Duplicates in DataFrames\n",
    "duplicate_posts = df_posts[df_posts.duplicated(subset=[\"id\"], keep=False)]\n",
    "duplicate_comments = df_comments[df_comments.duplicated(subset=[\"comment_id\"], keep=False)]\n",
    "\n",
    "\n",
    "print(df_comments.head())\n",
    "\n",
    "if duplicate_posts.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate posts found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate posts detected!\")\n",
    "    print(duplicate_posts)\n",
    "\n",
    "if duplicate_comments.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate comments found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate comments detected!\")\n",
    "    print(duplicate_comments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ Database Design\n",
    "\n",
    "Now that our **data is cleaned** and **sentiment scores are added**, we will:\n",
    "- **Define the SQLite database structure**\n",
    "- **Create tables (`posts` & `comments`) with foreign key relationships**\n",
    "- **Store the cleaned data into the database**\n",
    "\n",
    "### \n",
    "**Database Structure**\n",
    "We will store the data in **`data/reddit_data.db`**.\n",
    "\n",
    "| **Table**   | **Columns** | **Primary Key** | **Foreign Key** |\n",
    "|------------|------------|----------------|----------------|\n",
    "| **posts**  | `id, subreddit, title, score, num_comments, created_utc, text, url` | `id` | - |\n",
    "| **comments** | `comment_id, post_id, body, score, created_utc, comment_sentiment` | `comment_id` | `post_id` (FK ‚Üí posts.id) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Database Creation & Data Insertion Completed!\n"
     ]
    }
   ],
   "source": [
    "# üì• Step 4: Database Creation\n",
    "\n",
    "# Define database path\n",
    "DB_PATH = os.path.join(BASE_DIR, \"data\", \"reddit_data.db\")\n",
    "\n",
    "# Connect to SQLite and create tables\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create 'posts' table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS posts (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    subreddit TEXT,\n",
    "    title TEXT,\n",
    "    score INTEGER,\n",
    "    num_comments INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    text TEXT,\n",
    "    url TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create 'comments' table with sentiment analysis\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS comments (\n",
    "    comment_id TEXT PRIMARY KEY,\n",
    "    post_id TEXT,\n",
    "    body TEXT,\n",
    "    score INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    comment_sentiment REAL,\n",
    "    FOREIGN KEY (post_id) REFERENCES posts (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into SQLite database\n",
    "df_posts.to_sql(\"posts\", conn, if_exists=\"replace\", index=False)\n",
    "df_comments.to_sql(\"comments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\n‚úÖ Database Creation & Data Insertion Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check\n",
    "\n",
    "Before moving to analysis, we ensure **database integrity** by:\n",
    "- Checking record counts\n",
    "- Validating foreign key relationships\n",
    "- Inspecting sentiment score distribution\n",
    "\n",
    "This ensures **clean, structured data** for analysis in NB03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Table Row Counts:\n",
      "Posts: 84\n",
      "Comments: 2896\n",
      "\n",
      "‚úÖ Foreign Key Check: All comments have valid posts.\n",
      "\n",
      "üìä Post Count by Subreddit:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PoliticalDiscussion</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conservative</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit  post_count\n",
       "0  PoliticalDiscussion          50\n",
       "1         Conservative          27\n",
       "2             politics           7"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Quality Check Completed!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 5: Quality Check\n",
    "\n",
    "# Check Table Row Counts\n",
    "print(\"\\nüìä Table Row Counts:\")\n",
    "print(\"Posts:\", pd.read_sql_query(\"SELECT COUNT(*) FROM posts;\", conn).iloc[0, 0])\n",
    "print(\"Comments:\", pd.read_sql_query(\"SELECT COUNT(*) FROM comments;\", conn).iloc[0, 0])\n",
    "\n",
    "# Validate Foreign Keys (Ensure All Comments Link to Valid Posts)\n",
    "invalid_comments = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) FROM comments c\n",
    "    LEFT JOIN posts p ON c.post_id = p.id\n",
    "    WHERE p.id IS NULL;\n",
    "\"\"\", conn).iloc[0, 0]\n",
    "\n",
    "if invalid_comments == 0:\n",
    "    print(\"\\n‚úÖ Foreign Key Check: All comments have valid posts.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {invalid_comments} comments have no associated post!\")\n",
    "\n",
    "# ‚úÖ Count Posts Per Subreddit\n",
    "df_subreddit_counts = pd.read_sql_query(\"\"\"\n",
    "    SELECT subreddit, COUNT(*) AS post_count\n",
    "    FROM posts\n",
    "    GROUP BY subreddit\n",
    "    ORDER BY post_count DESC;\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nüìä Post Count by Subreddit:\")\n",
    "display(df_subreddit_counts)\n",
    "\n",
    "\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Data Quality Check Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
