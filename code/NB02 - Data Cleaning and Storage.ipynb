{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸŽ¯ Objective\n",
    "This notebook processes raw Reddit JSON data, cleans it, and saves it into a structured SQLite database. \n",
    "We will create two linked tables: 'posts' and 'comments'.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“š Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # File operations\n",
    "import json  # Handling JSON data\n",
    "import sqlite3  # Database storage\n",
    "import pandas as pd  # Data processing & transformation\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“Š Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Define correct file paths\n",
    "BASE_DIR = \"/files/ds105a-2024-alternative-summative-ajchan03\"  \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")  \n",
    "\n",
    "# Define file paths\n",
    "POSTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_posts.json\")\n",
    "COMMENTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_comments.json\")\n",
    "\n",
    "# Check if JSON files exist before attempting to load them\n",
    "if not os.path.exists(POSTS_FILE):\n",
    "    raise FileNotFoundError(f\"ðŸš¨ Error: `{POSTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "if not os.path.exists(COMMENTS_FILE):\n",
    "    raise FileNotFoundError(f\"ðŸš¨ Error: `{COMMENTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "# Load JSON data into DataFrames\n",
    "with open(POSTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_data = json.load(f)\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "\n",
    "with open(COMMENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    comments_data = json.load(f)\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Data successfully loaded\n",
    "print(\"Data Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ§¹ Data Cleaning & Transformation\n",
    "\n",
    "After loading the raw data, we need to **clean and transform it** to ensure its quality.\n",
    "\n",
    "### âœ… **Key Cleaning Tasks**\n",
    "1ï¸âƒ£ **Handle missing values** â†’ Fill or remove missing fields  \n",
    "2ï¸âƒ£ **Convert data types** â†’ Ensure `created_utc` fields are in `datetime` format  \n",
    "3ï¸âƒ£ **Remove duplicates** â†’ Ensure each post and comment is unique  \n",
    "4ï¸âƒ£ **Ensure data consistency** â†’ Ensure all comments link to valid posts  \n",
    "5ï¸âƒ£ **Perform sentiment analysis** â†’ Use **NLTK's VADER** to assign sentiment scores to each comment  \n",
    "\n",
    "### ðŸ“Œ **New Features Added**\n",
    "- **`comment_sentiment`** â†’ A numerical score representing the sentiment of a comment.  \n",
    "  - **Positive sentiment** â†’ Score **> 0.05**\n",
    "  - **Neutral sentiment** â†’ Score **between -0.05 and 0.05**\n",
    "  - **Negative sentiment** â†’ Score **< -0.05**  \n",
    "\n",
    "By the end of this step, our data will be **fully cleaned and enhanced with sentiment scores** for deeper analysis in the next steps.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Removed 51 duplicate posts\n",
      "\n",
      "âœ… Removed 3769 duplicate comments\n",
      "\n",
      "âœ… Foreign Key Consistency Check: 3403 comments remain after filtering\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/datahub/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Added Sentiment Score: `comment_sentiment` for comments\n",
      "\n",
      "ðŸ”Ž Final Missing Value Check (Should be 0s)\n",
      "id              0\n",
      "subreddit       0\n",
      "title           0\n",
      "score           0\n",
      "num_comments    0\n",
      "created_utc     0\n",
      "text            0\n",
      "url             0\n",
      "dtype: int64\n",
      "post_id              0\n",
      "comment_id           0\n",
      "body                 0\n",
      "score                0\n",
      "created_utc          0\n",
      "comment_sentiment    0\n",
      "dtype: int64\n",
      "\n",
      "ðŸ“Š Final Data Shape\n",
      "Posts: (49, 8)\n",
      "Comments: (3403, 6)\n",
      "\n",
      "ðŸ“Š Cleaned Comments Data Sample with Sentiment Score:\n",
      "                                                body  comment_sentiment\n",
      "0     Does this mean he canâ€™t even vote for himself?             0.0000\n",
      "1  I wonder if Trump is regretting the \" Lock her...             0.5101\n",
      "2                        He won 100% of the vote lol             0.7579\n",
      "3  He fucked a porn star. Then, he got fucked by ...            -0.8689\n",
      "4  Remember when he said Hillary should drop out ...            -0.2732\n",
      "\n",
      "âœ… Step 2: Data Cleaning & Sentiment Analysis Completed!\n"
     ]
    }
   ],
   "source": [
    "# ðŸš€ Step 2: Data Cleaning & Sentiment Analysis\n",
    "\n",
    "\n",
    "\n",
    "# Download the VADER sentiment analysis tool\n",
    "nltk.download(\"vader_lexicon\")\n",
    "\n",
    "# Initialize Sentiment Analyzer\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# ðŸ§¹ Handle Missing Values\n",
    "df_posts.fillna(\"\", inplace=True)\n",
    "df_comments.fillna(\"\", inplace=True)\n",
    "\n",
    "# âœ… Convert Date Fields to Datetime\n",
    "df_posts[\"created_utc\"] = pd.to_datetime(df_posts[\"created_utc\"])\n",
    "df_comments[\"created_utc\"] = pd.to_datetime(df_comments[\"created_utc\"])\n",
    "\n",
    "# âœ… Remove Duplicate Posts\n",
    "initial_post_count = len(df_posts)\n",
    "df_posts.drop_duplicates(subset=[\"id\"], keep=\"first\", inplace=True)\n",
    "print(f\"\\nâœ… Removed {initial_post_count - len(df_posts)} duplicate posts\")\n",
    "\n",
    "# âœ… Remove Duplicate Comments\n",
    "initial_comment_count = len(df_comments)\n",
    "df_comments.drop_duplicates(subset=[\"comment_id\"], keep=\"first\", inplace=True)\n",
    "print(f\"\\nâœ… Removed {initial_comment_count - len(df_comments)} duplicate comments\")\n",
    "\n",
    "# âœ… Ensure Foreign Key Consistency\n",
    "df_comments = df_comments[df_comments[\"post_id\"].isin(df_posts[\"id\"])]\n",
    "print(f\"\\nâœ… Foreign Key Consistency Check: {len(df_comments)} comments remain after filtering\")\n",
    "\n",
    "# âœ… Add Sentiment Analysis for Comments\n",
    "df_comments[\"comment_sentiment\"] = df_comments[\"body\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "print(\"\\nâœ… Added Sentiment Score: `comment_sentiment` for comments\")\n",
    "\n",
    "# âœ… Final Data Validation\n",
    "print(\"\\nðŸ”Ž Final Missing Value Check (Should be 0s)\")\n",
    "print(df_posts.isnull().sum())\n",
    "print(df_comments.isnull().sum())\n",
    "\n",
    "print(\"\\nðŸ“Š Final Data Shape\")\n",
    "print(f\"Posts: {df_posts.shape}\")\n",
    "print(f\"Comments: {df_comments.shape}\")\n",
    "\n",
    "# âœ… Display Cleaned Data Samples\n",
    "print(\"\\nðŸ“Š Cleaned Comments Data Sample with Sentiment Score:\")\n",
    "print(df_comments[[\"body\", \"comment_sentiment\"]].head())\n",
    "\n",
    "print(\"\\nâœ… Step 2: Data Cleaning & Sentiment Analysis Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ’¾ Database Design\n",
    "\n",
    "Now that our **data is cleaned** and **sentiment scores are added**, we will:\n",
    "- **Define the SQLite database structure**\n",
    "- **Create tables (`posts` & `comments`) with foreign key relationships**\n",
    "- **Store the cleaned data into the database**\n",
    "\n",
    "### **Database Structure**\n",
    "We will store the data in **`data/reddit_data.db`**.\n",
    "\n",
    "| **Table**   | **Columns** | **Primary Key** | **Foreign Key** |\n",
    "|------------|------------|----------------|----------------|\n",
    "| **posts**  | `id, subreddit, title, score, num_comments, created_utc, text, url` | `id` | - |\n",
    "| **comments** | `comment_id, post_id, body, score, created_utc, comment_sentiment` | `comment_id` | `post_id` (FK â†’ posts.id) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ðŸ“¥ Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "âœ… Database Creation & Data Insertion Completed!\n"
     ]
    }
   ],
   "source": [
    "# ðŸ“¥ Step 4: Database Creation\n",
    "\n",
    "# Define database path\n",
    "DB_PATH = os.path.join(BASE_DIR, \"data\", \"reddit_data.db\")\n",
    "\n",
    "# Connect to SQLite and create tables\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create 'posts' table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS posts (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    subreddit TEXT,\n",
    "    title TEXT,\n",
    "    score INTEGER,\n",
    "    num_comments INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    text TEXT,\n",
    "    url TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create 'comments' table with sentiment analysis\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS comments (\n",
    "    comment_id TEXT PRIMARY KEY,\n",
    "    post_id TEXT,\n",
    "    body TEXT,\n",
    "    score INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    comment_sentiment REAL,\n",
    "    FOREIGN KEY (post_id) REFERENCES posts (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into SQLite database\n",
    "df_posts.to_sql(\"posts\", conn, if_exists=\"replace\", index=False)\n",
    "df_comments.to_sql(\"comments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\nâœ… Database Creation & Data Insertion Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check\n",
    "\n",
    "Before moving to analysis, we ensure **database integrity** by:\n",
    "- Checking record counts\n",
    "- Validating foreign key relationships\n",
    "- Inspecting sentiment score distribution\n",
    "\n",
    "This ensures **clean, structured data** for analysis in NB03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ðŸ“Š Table Row Counts:\n",
      "Posts: 49\n",
      "Comments: 3403\n",
      "\n",
      "âœ… Foreign Key Check: All comments have valid posts.\n",
      "\n",
      "âœ… Data Quality Check Completed!\n"
     ]
    }
   ],
   "source": [
    "# âœ… Step 5: Quality Check\n",
    "\n",
    "# Check Table Row Counts\n",
    "print(\"\\nðŸ“Š Table Row Counts:\")\n",
    "print(\"Posts:\", pd.read_sql_query(\"SELECT COUNT(*) FROM posts;\", conn).iloc[0, 0])\n",
    "print(\"Comments:\", pd.read_sql_query(\"SELECT COUNT(*) FROM comments;\", conn).iloc[0, 0])\n",
    "\n",
    "# Validate Foreign Keys (Ensure All Comments Link to Valid Posts)\n",
    "invalid_comments = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) FROM comments c\n",
    "    LEFT JOIN posts p ON c.post_id = p.id\n",
    "    WHERE p.id IS NULL;\n",
    "\"\"\", conn).iloc[0, 0]\n",
    "\n",
    "if invalid_comments == 0:\n",
    "    print(\"\\nâœ… Foreign Key Check: All comments have valid posts.\")\n",
    "else:\n",
    "    print(f\"\\nâš ï¸ Warning: {invalid_comments} comments have no associated post!\")\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "print(\"\\nâœ… Data Quality Check Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
