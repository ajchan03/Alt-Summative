{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üéØ Objective\n",
    "\n",
    "This notebook processes raw Reddit data collected in NB01, cleans it, and stores it in a structured SQLite database.  \n",
    "We are analyzing discussions from three different subreddits:  \n",
    "- **r/politics** (left-leaning)\n",
    "- **r/Conservative** (right-leaning)\n",
    "- **r/PoliticalDiscussion** (moderated, neutral discourse)\n",
    "\n",
    "By the end of this notebook, we will have **a structured dataset** ready for analysis in **NB03**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìö Libraries\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  # File operations\n",
    "import json  # Handling JSON data\n",
    "import sqlite3  # Database storage\n",
    "import pandas as pd  # Data processing & transformation\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loaded Successfully\n"
     ]
    }
   ],
   "source": [
    "# Define correct file paths\n",
    "BASE_DIR = \"/files/ds105a-2024-alternative-summative-ajchan03\"  \n",
    "DATA_DIR = os.path.join(BASE_DIR, \"data\", \"raw\")  \n",
    "\n",
    "# Define file paths\n",
    "POSTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_posts.json\")\n",
    "COMMENTS_FILE = os.path.join(DATA_DIR, \"reddit_filtered_comments.json\")\n",
    "\n",
    "# Check if JSON files exist before attempting to load them\n",
    "if not os.path.exists(POSTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{POSTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "if not os.path.exists(COMMENTS_FILE):\n",
    "    raise FileNotFoundError(f\"üö® Error: `{COMMENTS_FILE}` not found. Please run the scraper first.\")\n",
    "\n",
    "# Load JSON data into DataFrames\n",
    "with open(POSTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    posts_data = json.load(f)\n",
    "df_posts = pd.DataFrame(posts_data)\n",
    "\n",
    "with open(COMMENTS_FILE, \"r\", encoding=\"utf-8\") as f:\n",
    "    comments_data = json.load(f)\n",
    "df_comments = pd.DataFrame(comments_data)\n",
    "\n",
    "# Data successfully loaded\n",
    "print(\"Data Loaded Successfully\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üßπ Data Cleaning & Transformation\n",
    "\n",
    "We ensure our dataset is **clean and structured** by:\n",
    "- Handling **missing values**  \n",
    "- Converting timestamps to **datetime format**  \n",
    "- Removing **duplicate posts and comments**  \n",
    "- Ensuring all comments **link to a valid post**  \n",
    "- **Filtering only posts from `r/politics`, `r/Conservative`, `SandersForPresident`, and `r/PoliticalDiscussion`**  \n",
    "- **Filters comments so that only ones containing 'Trump' remain**\n",
    "- Adding **sentiment analysis** to comments  \n",
    "\n",
    "By the end of this step, our dataset will be **structured for database storage**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/datahub/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   post_id comment_id                                               body  \\\n",
      "1  1d4emcb    l6dw3k9  r/Conservative can finally celebrate Trump bei...   \n",
      "3  1d4emcb    l6ds5i1  After the 60+ election interference cases Trum...   \n",
      "4  1d4emcb    l6dtmrl  Twice impeached and 34-time convicted felon, D...   \n",
      "5  1d4emcb    l6dxmop  Convicted Felon, Fraud and Rapist, former Pres...   \n",
      "8  1d4emcb    l6dqv5k  Donald Trump is a winner, got the full 34 coun...   \n",
      "\n",
      "   score         created_utc subreddit  comment_sentiment  \n",
      "1     35 2024-05-30 21:43:19  politics             0.9117  \n",
      "3   1938 2024-05-30 21:20:32  politics            -0.4215  \n",
      "4     40 2024-05-30 21:29:05  politics             0.0000  \n",
      "5     43 2024-05-30 21:52:19  politics            -0.7845  \n",
      "8    297 2024-05-30 21:13:07  politics             0.1099  \n",
      "\n",
      "‚úîÔ∏è No duplicate posts found.\n",
      "\n",
      "‚úîÔ∏è No duplicate comments found.\n"
     ]
    }
   ],
   "source": [
    "# üöÄ Step 2: Data Cleaning (Filtering for Trump-Related Comments)\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# Download NLTK VADER sentiment analysis tool\n",
    "nltk.download(\"vader_lexicon\")\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "# üßπ Handle Missing Values\n",
    "df_posts.fillna(\"\", inplace=True)\n",
    "df_comments.fillna(\"\", inplace=True)\n",
    "\n",
    "# ‚úÖ Convert Date Fields to Datetime\n",
    "df_posts[\"created_utc\"] = pd.to_datetime(df_posts[\"created_utc\"])\n",
    "df_comments[\"created_utc\"] = pd.to_datetime(df_comments[\"created_utc\"])\n",
    "\n",
    "# Add 'subreddit' to Comments Table\n",
    "df_comments = df_comments.merge(df_posts[['id', 'subreddit']], left_on=\"post_id\", right_on=\"id\", how=\"left\")\n",
    "\n",
    "# Drop the duplicate 'id' column (from df_posts) since 'post_id' is already in df_comments\n",
    "df_comments.drop(columns=[\"id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Remove Duplicate Posts\n",
    "df_posts.drop_duplicates(subset=[\"id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Remove Duplicate Comments\n",
    "df_comments.drop_duplicates(subset=[\"comment_id\"], inplace=True)\n",
    "\n",
    "# ‚úÖ Ensure Foreign Key Consistency\n",
    "df_comments = df_comments[df_comments[\"post_id\"].isin(df_posts[\"id\"])]\n",
    "\n",
    "# ‚úÖ Filter Comments: Keep only those mentioning \"Trump\" (case-insensitive)\n",
    "df_comments = df_comments[df_comments[\"body\"].str.contains(r'\\bTrump\\b', flags=re.IGNORECASE, regex=True, na=False)]\n",
    "\n",
    "# ‚úÖ Add Sentiment Analysis\n",
    "df_comments[\"comment_sentiment\"] = df_comments[\"body\"].apply(lambda text: sia.polarity_scores(text)[\"compound\"])\n",
    "\n",
    "# Ensure No Duplicates in DataFrames\n",
    "duplicate_posts = df_posts[df_posts.duplicated(subset=[\"id\"], keep=False)]\n",
    "duplicate_comments = df_comments[df_comments.duplicated(subset=[\"comment_id\"], keep=False)]\n",
    "\n",
    "\n",
    "print(df_comments.head())\n",
    "\n",
    "if duplicate_posts.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate posts found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate posts detected!\")\n",
    "    print(duplicate_posts)\n",
    "\n",
    "if duplicate_comments.empty:\n",
    "    print(\"\\n‚úîÔ∏è No duplicate comments found.\")\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è Duplicate comments detected!\")\n",
    "    print(duplicate_comments)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üíæ Database Design\n",
    "\n",
    "Now that our **data is cleaned** and **sentiment scores are added**, we will:\n",
    "- **Define the SQLite database structure**\n",
    "- **Create tables (`posts` & `comments`) with foreign key relationships**\n",
    "- **Store the cleaned data into the database**\n",
    "\n",
    "### \n",
    "**Database Structure**\n",
    "We will store the data in **`data/reddit_data.db`**.\n",
    "\n",
    "| **Table**   | **Columns** | **Primary Key** | **Foreign Key** |\n",
    "|------------|------------|----------------|----------------|\n",
    "| **posts**  | `id, subreddit, title, score, num_comments, created_utc, text, url` | `id` | - |\n",
    "| **comments** | `comment_id, post_id, body, score, created_utc, comment_sentiment` | `comment_id` | `post_id` (FK ‚Üí posts.id) |\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üì• Database Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Database Creation & Data Insertion Completed!\n"
     ]
    }
   ],
   "source": [
    "# üì• Step 4: Database Creation\n",
    "\n",
    "# Define database path\n",
    "DB_PATH = os.path.join(BASE_DIR, \"data\", \"reddit_data.db\")\n",
    "\n",
    "# Connect to SQLite and create tables\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Create 'posts' table\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS posts (\n",
    "    id TEXT PRIMARY KEY,\n",
    "    subreddit TEXT,\n",
    "    title TEXT,\n",
    "    score INTEGER,\n",
    "    num_comments INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    text TEXT,\n",
    "    url TEXT\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Create 'comments' table with sentiment analysis\n",
    "cursor.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS comments (\n",
    "    comment_id TEXT PRIMARY KEY,\n",
    "    post_id TEXT,\n",
    "    body TEXT,\n",
    "    score INTEGER,\n",
    "    created_utc DATETIME,\n",
    "    comment_sentiment REAL,\n",
    "    FOREIGN KEY (post_id) REFERENCES posts (id)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data into SQLite database\n",
    "df_posts.to_sql(\"posts\", conn, if_exists=\"replace\", index=False)\n",
    "df_comments.to_sql(\"comments\", conn, if_exists=\"replace\", index=False)\n",
    "\n",
    "conn.commit()\n",
    "print(\"\\n‚úÖ Database Creation & Data Insertion Completed!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quality Check\n",
    "\n",
    "Before moving to analysis, we ensure **database integrity** by:\n",
    "- Checking record counts\n",
    "- Validating foreign key relationships\n",
    "- Inspecting sentiment score distribution\n",
    "\n",
    "This ensures **clean, structured data** for analysis in NB03.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìä Table Row Counts:\n",
      "Posts: 89\n",
      "Comments: 2968\n",
      "\n",
      "‚úÖ Foreign Key Check: All comments have valid posts.\n",
      "\n",
      "üìä Post Count by Subreddit:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>subreddit</th>\n",
       "      <th>post_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>PoliticalDiscussion</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Conservative</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>politics</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SandersForPresident</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             subreddit  post_count\n",
       "0  PoliticalDiscussion          50\n",
       "1         Conservative          27\n",
       "2             politics           7\n",
       "3  SandersForPresident           5"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "‚úÖ Data Quality Check Completed!\n"
     ]
    }
   ],
   "source": [
    "# ‚úÖ Step 5: Quality Check\n",
    "\n",
    "# Check Table Row Counts\n",
    "print(\"\\nüìä Table Row Counts:\")\n",
    "print(\"Posts:\", pd.read_sql_query(\"SELECT COUNT(*) FROM posts;\", conn).iloc[0, 0])\n",
    "print(\"Comments:\", pd.read_sql_query(\"SELECT COUNT(*) FROM comments;\", conn).iloc[0, 0])\n",
    "\n",
    "# Validate Foreign Keys (Ensure All Comments Link to Valid Posts)\n",
    "invalid_comments = pd.read_sql_query(\"\"\"\n",
    "    SELECT COUNT(*) FROM comments c\n",
    "    LEFT JOIN posts p ON c.post_id = p.id\n",
    "    WHERE p.id IS NULL;\n",
    "\"\"\", conn).iloc[0, 0]\n",
    "\n",
    "if invalid_comments == 0:\n",
    "    print(\"\\n‚úÖ Foreign Key Check: All comments have valid posts.\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è Warning: {invalid_comments} comments have no associated post!\")\n",
    "\n",
    "# ‚úÖ Count Posts Per Subreddit\n",
    "df_subreddit_counts = pd.read_sql_query(\"\"\"\n",
    "    SELECT subreddit, COUNT(*) AS post_count\n",
    "    FROM posts\n",
    "    GROUP BY subreddit\n",
    "    ORDER BY post_count DESC;\n",
    "\"\"\", conn)\n",
    "\n",
    "print(\"\\nüìä Post Count by Subreddit:\")\n",
    "display(df_subreddit_counts)\n",
    "\n",
    "\n",
    "\n",
    "# Close Database Connection\n",
    "conn.close()\n",
    "print(\"\\n‚úÖ Data Quality Check Completed!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
